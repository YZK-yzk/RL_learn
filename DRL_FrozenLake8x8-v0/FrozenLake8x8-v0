import tensorflow as tf
import numpy as np
import gym

tf.set_random_seed(1)
np.random.seed(1)

NSTATUS=1
class DQN():
    def __init__(self,nstate,naction):
        self.nstate=nstate
        self.naction=naction
        self.sess = tf.Session()
        self.memcnt=0
        self.BATCH_SIZE = 64
        self.LR = 0.001  # learning rate
        self.EPSILON = 0.95  # greedy policy
        self.GAMMA = 0.9  # reward discount
        self.MEM_CAP = 20000
        #self.mem =np.array([[0]*self.naction]*self.MEM_CAP)
        self.mem= np.zeros((self.MEM_CAP, NSTATUS * 2 + 2))     # initialize memory
        self.updataT=100
        self.built_net()


    def built_net(self):
        self.s = tf.placeholder(tf.float64, [None,NSTATUS])
        self.a = tf.placeholder(tf.int32, [None,])
        self.r = tf.placeholder(tf.float64, [None,])
        self.s_ = tf.placeholder(tf.float64, [None,NSTATUS])

        with tf.variable_scope('q'):                 # evaluation network
            l_eval = tf.layers.dense(self.s, 10, tf.nn.relu, kernel_initializer=tf.random_normal_initializer(0, 0.1))
            self.q = tf.layers.dense(l_eval, self.naction, kernel_initializer=tf.random_normal_initializer(0, 0.1))

        with tf.variable_scope('q_next'):  # target network, not to train
            l_target = tf.layers.dense(self.s_, 10, tf.nn.relu, trainable=False)
            q_next = tf.layers.dense(l_target, self.naction, trainable=False)

        q_target = self.r + self.GAMMA * tf.reduce_max(q_next, axis=1)    #q_next:  shape=(None, naction),
        a_index=tf.stack([tf.range(self.BATCH_SIZE,dtype=tf.int32),self.a],axis=1)
        q_eval=tf.gather_nd(params=self.q,indices=a_index)
        loss=tf.losses.mean_squared_error(q_target,q_eval)
        self.train=tf.train.AdamOptimizer(self.LR).minimize(loss)
        #  q现实target_net- Q估计
        self.sess.run(tf.global_variables_initializer())

    def choose_action(self,status):
        status=np.array([status]).reshape((1,NSTATUS))
        if np.random.uniform(0,1)<self.EPSILON:
            action=np.argmax( self.sess.run(self.q,feed_dict={self.s:status}))
        else:
            action=np.random.randint(0,self.naction)
        return action

    def learn(self):
        if(self.memcnt%self.updataT==0):
            t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_next')
            e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q')
            self.sess.run([tf.assign(t, e) for t, e in zip(t_params, e_params)])
        rand_indexs=np.random.choice(self.MEM_CAP,self.BATCH_SIZE,replace=False)
        temp=self.mem[rand_indexs]
        bs = temp[:,0:NSTATUS].reshape(self.BATCH_SIZE,NSTATUS)
        ba = temp[:,NSTATUS]
        br = temp[:,NSTATUS+1]
        bs_ = temp[:,NSTATUS+2:].reshape(self.BATCH_SIZE,NSTATUS)
        self.sess.run(self.train, feed_dict={self.s:bs,self.a:ba,self.r:br,self.s_:bs_})


    def storeExp(self,s,a,r,s_):
        self.mem[self.memcnt%self.MEM_CAP]=np.hstack([s,a,r,s_])
        self.memcnt+=1


    def run(self):
        cnt_win =1
        cnt_lost=1
        for i in range(10000):
            sr=0
            s=env.reset()
            done=False
            while(not done):
                a=self.choose_action(s)
                s_,r,done,_=env.step(a)
                self.storeExp(s,a,r,s_)
                if(self.memcnt>self.MEM_CAP):
                    self.learn()
                    if(done):
                        if(s_==self.nstate-1):
                            #env.render()
                            cnt_win+=1.0
                            #print("succeed!")
                        else:
                            #env.render()
                            cnt_lost+=1.0
                            #print("lost in the hold!")
                s=s_
            if (i % 50 == 0):
                print(i, ": ")
                print(cnt_win, cnt_win / cnt_lost)


#env = gym.make('CartPole-v0')
env = gym.make('FrozenLake-v0')
env = env.unwrapped
dqn=DQN(env.observation_space.n,env.action_space.n)
dqn.run()

















